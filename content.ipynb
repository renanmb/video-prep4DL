{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "47c7f972",
   "metadata": {},
   "source": [
    "# Hardware Accelerated Video Preprocessing for Deep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "087ec891",
   "metadata": {},
   "source": [
    "### Video Preprocessing\n",
    "\n",
    "Video is a rich source of information for artificial intelligence (AI) applications, and is used in a wide range of industries and domains. Some of the most common use cases for video in AI include:\n",
    "\n",
    " - Computer Vision= AI algorithms can analyze video streams to detect and recognize objects, faces, and actions, which has applications in security and surveillance, autonomous vehicles, and video content analysis.\n",
    "\n",
    " - Video Analytics= AI algorithms can process video data to extract valuable insights, such as customer behavior and preferences, which can be used in marketing and customer experience management.\n",
    "\n",
    " - Video Content Creation= AI can be used to generate or enhance video content, such as creating 3D models, adding special effects, or synthesizing new images and videos.\n",
    "\n",
    " - Video Streaming= AI can be used to optimize video delivery, such as by reducing bandwidth consumption, improving quality of experience, or enabling new use cases like virtual reality and augmented reality.\n",
    "\n",
    "These are just a few examples of the many ways that AI is being applied to video to create new and innovative solutions for a variety of industries and domains."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f2e221f",
   "metadata": {},
   "source": [
    "#### Decoding\n",
    "\n",
    "Video decoding is the process of converting compressed video data into a displayable format. The compressed data is first decompressed and then decoded into separate image frames that can be displayed on a screen. This process involves several steps, including entropy decoding, inverse quantization, and inverse discrete cosine transform. \n",
    "\n",
    "Fortunatelly, there are ready to use solutions to perform video decoding. Code below shows how to decode video in Python by using OpenCV,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ded2f4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Video\n",
    "\n",
    "video_path = 'data/videos/test.mp4'\n",
    "\n",
    "Video(video_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "064dabb4",
   "metadata": {},
   "source": [
    "##### Example of using FFMPEG via OpenCV `VideoCapture`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d386f949",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "from image_utils import show_images\n",
    "\n",
    "# Open video file by using OpenCV\n",
    "video =  cv2.VideoCapture(video_path)\n",
    "\n",
    "print('Backend used to decode the video: ', video.getBackendName())\n",
    "\n",
    "# Read first 8 frames\n",
    "frames =  []\n",
    "for _ in range(8):\n",
    "    ret, frame =  video.read()\n",
    "    frame =  cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    frames.append(frame)\n",
    "\n",
    "show_images(frames)\n",
    "\n",
    "# Clenup\n",
    "video.release()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a5e8a3c",
   "metadata": {},
   "source": [
    "Connecting the previous example with Pytorch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78310d07",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "video =  cv2.VideoCapture(video_path)\n",
    "device = torch.device('cuda')\n",
    "\n",
    "frames = []\n",
    "for _ in range(8):\n",
    "    ret, frame = video.read()\n",
    "    frame =  cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    tensor = torch.from_numpy(frame)\n",
    "    tensor = tensor.to(device)\n",
    "    tensor = tensor.float()\n",
    "\n",
    "    mean = torch.mean(tensor)\n",
    "    std = torch.std(tensor)\n",
    "    tensor = (tensor - mean) / std\n",
    "\n",
    "    frames.append(tensor)\n",
    "\n",
    "\n",
    "batch = torch.stack(frames)\n",
    "print(batch.size())\n",
    "\n",
    "# Pass batch to model...\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcfe9c7e",
   "metadata": {},
   "source": [
    "### Hardware accelerated"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fd8074f",
   "metadata": {},
   "source": [
    "The NVIDIA Video Codec SDK is a set of tools and APIs that enables hardware accelerated video decoding on NVIDIA GPUs. It supports a wide range of video codecs including H.264, HEVC, VP9 and AV1, and allows for efficient decoding of high-resolution and high-bitrate video streams. The hardware decoding capabilities of the NVIDIA Video Codec SDK can significantly reduce CPU usage and power consumption, making it ideal for use in high-performance computing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e81eb6a",
   "metadata": {},
   "source": [
    "#### Decoding\n",
    "\n",
    "The NVIDIA Video Processing Framework (VPF) is a software library for video processing and video analysis. It provides a set of tools and APIs to perform various video processing tasks, such as video decoding, encoding, transcoding, and video analysis. VPF leverages the power of NVIDIA GPUs to accelerate video processing and analysis tasks, allowing for real-time performance on demanding video workloads. The framework supports a wide range of video codecs and formats, including H.264, HEVC, VP9, and AV1. VPF is designed for use in a variety of applications, including video streaming, media and entertainment, security and surveillance, and autonomous vehicles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00d781a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import PyNvCodec as nvc\n",
    "import numpy as np\n",
    "\n",
    "decoder =  nvc.PyNvDecoder(video_path, 0)\n",
    "\n",
    "raw_surface =  decoder.DecodeSingleSurface()\n",
    "print(raw_surface)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e846df5",
   "metadata": {},
   "source": [
    "How to convert raw surface to RGB and move it to the CPU. First we need to prepare some objects:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d5d7d29",
   "metadata": {},
   "outputs": [],
   "source": [
    "width =  decoder.Width()\n",
    "height =  decoder.Height()\n",
    "\n",
    "# To convert pixel format and color space\n",
    "to_rgb =  nvc.PySurfaceConverter(\n",
    "    width,\n",
    "    height,\n",
    "    nvc.PixelFormat.NV12,\n",
    "    nvc.PixelFormat.RGB,\n",
    "    0)\n",
    "\n",
    "to_rgb_context =  nvc.ColorspaceConversionContext(\n",
    "    nvc.ColorSpace.BT_709, nvc.ColorRange.JPEG)\n",
    "\n",
    "# To download to the CPU\n",
    "to_cpu =  nvc.PySurfaceDownloader(width, height, nvc.PixelFormat.RGB, 0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89fa26d1",
   "metadata": {},
   "source": [
    "And use them to process frames:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcc3ebe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "frames =  []\n",
    "for i in range(8):\n",
    "    raw_surface =  decoder.DecodeSingleSurface()\n",
    "    rgb_surface =  to_rgb.Execute(raw_surface, to_rgb_context)\n",
    "\n",
    "    frame =  np.ndarray(shape= (height, width, 3), dtype= np.uint8)\n",
    "    to_cpu.DownloadSingleSurface(rgb_surface, frame)\n",
    "    frames.append(frame)\n",
    "\n",
    "show_images(frames)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a849640",
   "metadata": {},
   "source": [
    "VPF integrates with PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "581a5b54",
   "metadata": {},
   "outputs": [],
   "source": [
    "import PytorchNvCodec as pnvc\n",
    "\n",
    "\n",
    "def surface_to_tensor(surface: nvc.Surface) -> torch.Tensor:\n",
    "    surf_plane = surface.PlanePtr()\n",
    "    img_tensor = pnvc.DptrToTensor(\n",
    "        surf_plane.GpuMem(),\n",
    "        surf_plane.Width(),\n",
    "        surf_plane.Height(),\n",
    "        surf_plane.Pitch(),\n",
    "        surf_plane.ElemSize(),\n",
    "    )\n",
    "    if img_tensor is None:\n",
    "        raise RuntimeError(\"Can not export to tensor.\")\n",
    "\n",
    "    img_tensor.resize_(3, surf_plane.Height(), int(surf_plane.Width() / 3))\n",
    "    img_tensor = img_tensor.type(dtype=torch.cuda.FloatTensor)\n",
    "\n",
    "    return img_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af581885",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(8):\n",
    "    raw_surface =  decoder.DecodeSingleSurface()\n",
    "    rgb_surface =  to_rgb.Execute(raw_surface, to_rgb_context)\n",
    "\n",
    "    tensor = surface_to_tensor(rgb_surface)\n",
    "    \n",
    "    print(tensor.type(), tensor.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6b13bd9",
   "metadata": {},
   "source": [
    "#### Benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd845ed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.torch_utils import time_synchronized\n",
    "\n",
    "video =  cv2.VideoCapture(video_path)\n",
    "num_frames = 50\n",
    "\n",
    "device = torch.device('cuda')\n",
    "\n",
    "t0 = time_synchronized()\n",
    "for _ in range(num_frames):\n",
    "    ret, frame = video.read()\n",
    "    frame =  cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    tensor = torch.from_numpy(frame)\n",
    "    tensor = tensor.to(device)\n",
    "    tensor = tensor.float()\n",
    "\n",
    "duration = time_synchronized() - t0\n",
    "print(f'cv2: {num_frames/duration:.3f} fps')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5ba5ef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder =  nvc.PyNvDecoder(video_path, 0)\n",
    "\n",
    "t0 = time_synchronized()\n",
    "for _ in range(num_frames):\n",
    "    raw_surface =  decoder.DecodeSingleSurface()\n",
    "    rgb_surface =  to_rgb.Execute(raw_surface, to_rgb_context)\n",
    "\n",
    "    tensor = surface_to_tensor(rgb_surface)\n",
    "\n",
    "duration = time_synchronized() - t0\n",
    "print(f'vpf: {num_frames/duration:.3f} fps')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4650c42",
   "metadata": {},
   "source": [
    "#### Processing\n",
    "\n",
    "NVIDIA DALI (Data Augmentation Library for Imaging) is a high-performance library for image pre-processing. It provides a simple and efficient interface for executing complex data augmentation pipelines on GPUs. DALI enables the fast and efficient creation of large and diverse datasets, which is critical for training deep learning models. DALI offers a wide range of augmentation operations, including cropping, scaling, flipping, rotation, and color correction, as well as support for custom operations and pipeline parallelism. Additionally, DALI integrates with popular deep learning frameworks such as TensorFlow and PyTorch, making it easy to incorporate into existing workflows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d0bf2dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nvidia.dali import pipeline_def\n",
    "import nvidia.dali.fn as fn\n",
    "\n",
    "device_id = 0\n",
    "\n",
    "# Define DALI pipeline\n",
    "@pipeline_def(batch_size= 1, num_threads= 2, device_id= device_id)\n",
    "def video_pipeline():\n",
    "    video =  fn.readers.video(\n",
    "        device= \"gpu\",\n",
    "        filenames= [video_path],\n",
    "        # step=2,\n",
    "        sequence_length= 8)\n",
    "    \n",
    "    # More operations and augmentations can be placed here\n",
    "\n",
    "    return video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55dd93cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create pipeline object\n",
    "pipeline =  video_pipeline()\n",
    "print(pipeline)\n",
    "\n",
    "# Build the pipeline. Operators are instantiated at this stage\n",
    "pipeline.build()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ea579ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run pipeline to get outputs\n",
    "frames =  pipeline.run()\n",
    "\n",
    "print(type(frames[0]))\n",
    "\n",
    "# Move frames to the CPU and display them\n",
    "host_frames =  frames[0].as_cpu().as_array()\n",
    "\n",
    "show_images(host_frames[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f0db5e2",
   "metadata": {},
   "source": [
    "### In Deep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b94d7f7",
   "metadata": {},
   "source": [
    "#### JAX\n",
    "\n",
    "Google JAX is a library for high-performance numerical computing and machine learning research, built on top of the popular NumPy library. JAX combines the best features of NumPy with the power of hardware-accelerated GPU and TPU computing, allowing for fast and efficient computation on large arrays and matrices. JAX also includes support for automatic differentiation, which enables gradient-based optimization of machine learning models. Additionally, JAX integrates with popular deep learning frameworks such as TensorFlow and PyTorch, making it easy to incorporate into existing workflows. JAX is designed to be flexible, allowing users to write code that runs on CPUs, GPUs, and TPUs with no code changes, and to take advantage of hardware acceleration with minimal overhead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b9d7750",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax.dlpack\n",
    "import cupy\n",
    "\n",
    "frames =  pipeline.run()\n",
    "\n",
    "def dali_tensor_to_jax(dali_tensor):\n",
    "    return jax.dlpack.from_dlpack(cupy.asarray(dali_tensor).toDlpack())\n",
    "\n",
    "# Move DALI output to JAX array object\n",
    "jax_frames =  dali_tensor_to_jax(frames[0][0])\n",
    "\n",
    "print(jax_frames.shape)\n",
    "print(jax_frames.sharding)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21691448",
   "metadata": {},
   "source": [
    "#### MNIST\n",
    "\n",
    "The MNIST dataset is a popular benchmark dataset in machine learning, consisting of 70,000 grayscale images of handwritten digits from 0 to 9, each with a resolution of 28x28 pixels. It is widely used for image classification tasks and has been used to train and evaluate various machine learning algorithms, including deep neural networks. The dataset is often used as a starting point for beginners to practice and explore different machine learning techniques, due to its simplicity and availability. The MNIST dataset has played a significant role in advancing the field of computer vision.\n",
    "\n",
    "Test video contains upsacled MNIST digits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45bca17c",
   "metadata": {},
   "outputs": [],
   "source": [
    "Video('data/videos/session_number.mp4')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eabbe6a3",
   "metadata": {},
   "source": [
    "In this example we will train simple model in JAX to recognize MNIST digits. We will use DALI pipeline to read and preproces MNIST dataset and serve it to the model training. Later we will write another pipeline to decode the test video and run inference on the trained model to recognize the numbers in the frames.\n",
    "\n",
    "First, code below defines a DALI pipeline for reading and preprocessing the MNIST dataset. The pipeline reads the images and labelss from the MNIST training directory, decodes the images, converts them to grayscale, and one-hot encodes the labels. The pipeline is set to have a batch size of 128, run on 2 threads, and use the GPU device with ID 0. The resulting output of the pipeline is a tuple of preprocessed images and their corresponding one-hot encoded labels, which can be used for training a machine learning model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3a30311",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nvidia.dali.types as types\n",
    "\n",
    "batch_size = 128\n",
    "image_size = 28\n",
    "\n",
    "@pipeline_def(batch_size=batch_size, num_threads=2, device_id=0)\n",
    "def mnist_pipeline():\n",
    "    jpegs, labels = fn.readers.caffe2(path='data/MNIST/training/', random_shuffle=True)\n",
    "    images = fn.decoders.image(\n",
    "        jpegs, device='mixed', output_type=types.GRAY)\n",
    "    labels = labels.gpu()\n",
    "    labels = fn.one_hot(labels, num_classes=10)\n",
    "\n",
    "    return images, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9955e7eb",
   "metadata": {},
   "source": [
    "Next there is a pipeline instantiation and building."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e03af828",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = mnist_pipeline()\n",
    "pipeline.build()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06fbf257",
   "metadata": {},
   "source": [
    "We can run the pipeline and visualise the outpus to see that they are correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c196415c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from image_utils import show_images_greyscale\n",
    "\n",
    "images, labels =  pipeline.run()\n",
    "\n",
    "host_frames =  images.as_cpu().as_array()\n",
    "show_images_greyscale(host_frames[0:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c33e8e01",
   "metadata": {},
   "source": [
    "Code below uses created DALI pipeline to train a simple JAX model to recognize MNIST digits. We display current accuracy on the training data every 500 iterations to visualise training progress."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab1938da",
   "metadata": {},
   "outputs": [],
   "source": [
    "from jax import numpy as jnp\n",
    "from mnist import init_params, update, accuracy\n",
    "\n",
    "# Init model\n",
    "params = init_params(scale=0.1, layer_sizes=[784, 1024, 1024, 10])\n",
    "\n",
    "# Train for 3000 iterations\n",
    "for i in range(3000):\n",
    "    images, labels = pipeline.run()\n",
    "\n",
    "    images = dali_tensor_to_jax(images.as_tensor()).reshape((batch_size, image_size * image_size))\n",
    "    labels = dali_tensor_to_jax(labels.as_tensor())\n",
    "\n",
    "    params = update(params, images, labels)\n",
    "\n",
    "    if (i % 500) == 0:\n",
    "        acc = accuracy(params, images, labels)\n",
    "        print(f'Accuracy at iteration {i}: {acc}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9bdfbf2",
   "metadata": {},
   "source": [
    "Last code sample in this example defines another DALI pipeline that will be used in inference. This pipeline uses hardware accelerated video decoding to get the frames and preprocesses them to match desired model input. After that they are passed to the model and we print the predicted classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c246e6d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mnist import predict\n",
    "\n",
    "@pipeline_def(batch_size= 1, num_threads= 2, device_id= 0)\n",
    "def video_pipeline():\n",
    "    video =  fn.readers.video(\n",
    "        device= \"gpu\",\n",
    "        filenames= ['data/videos/session_number.mp4'],\n",
    "        sequence_length= 5)\n",
    "    video = fn.color_space_conversion(video, image_type=types.RGB, output_type=types.GRAY)\n",
    "    video = fn.resize(video, resize_x = 28, resize_y = 28)\n",
    "\n",
    "    return video\n",
    "\n",
    "inference_pipeline = video_pipeline()\n",
    "inference_pipeline.build()\n",
    "\n",
    "for i in range(5):\n",
    "    frames = inference_pipeline.run()\n",
    "\n",
    "    frames = dali_tensor_to_jax(frames[0].as_tensor()).reshape((5, 28*28))\n",
    "    predicted_class = jnp.argmax(predict(params, frames), axis=1)\n",
    "    print(predicted_class)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b5d7767",
   "metadata": {},
   "source": [
    "#### YOLO v7\n",
    "\n",
    "YOLO (You Only Look Once) is a popular object detection model for real-time object detection and classification. YOLO divides an input image into a grid of cells, and uses a neural network to predict the presence and location of objects within each cell. The YOLO model is fast and efficient, making it well-suited for real-time object detection in video streams and other applications.\n",
    "\n",
    "YOLO v7 is a version of the YOLO model, and introduces several new features and improvements over previous versions. Some of the key features of YOLO v7 include= improved accuracy, multi-scale training, and improved architecture for detecting smaller objects. YOLO v7 also includes support for the latest advances in deep learning and computer vision, making it a powerful tool for object detection and classification tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cbf3ddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "Video('data/videos/test2.mp4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bbdf70e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nvidia.dali import pipeline_def\n",
    "import nvidia.dali.fn as fn\n",
    "import nvidia.dali.types as types\n",
    "\n",
    "device_id = 0\n",
    "\n",
    "@pipeline_def(batch_size=1, num_threads=2, device_id=device_id)\n",
    "def video_pipeline():\n",
    "    raw =  fn.readers.video(\n",
    "        name=\"reader\",\n",
    "        device=\"gpu\",\n",
    "        filenames=['data/videos/test2.mp4'],\n",
    "        sequence_length=8)\n",
    "    video = fn.resize(raw, resize_x=640, resize_y=360)\n",
    "    video = fn.crop(\n",
    "        video,\n",
    "        crop_h=360 + 24,\n",
    "        crop_w=640,\n",
    "        out_of_bounds_policy='pad',\n",
    "        fill_values=114.)\n",
    "    raw = fn.color_space_conversion(\n",
    "        raw, image_type=types.RGB, output_type=types.BGR)\n",
    "\n",
    "    return video, raw\n",
    "\n",
    "\n",
    "pipeline =  video_pipeline()\n",
    "pipeline.build()\n",
    "\n",
    "frames =  pipeline.run()\n",
    "host_frames =  frames[0].as_cpu().as_array()\n",
    "\n",
    "show_images(host_frames[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78cf9ba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nvidia.dali.plugin.pytorch import DALIGenericIterator\n",
    "\n",
    "pipeline = video_pipeline()\n",
    "pipeline.build()\n",
    "\n",
    "dali_pytorch_iterator = DALIGenericIterator([pipeline], ['frames', 'raw'], reader_name=\"reader\")\n",
    "\n",
    "for i, data in enumerate(dali_pytorch_iterator):\n",
    "    print(data[device_id]['frames'].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bdb10eb",
   "metadata": {},
   "source": [
    "##### YOLOv7 based object detection:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4b9cc8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from detect import detect\n",
    "\n",
    "dali_pytorch_iterator.reset()\n",
    "\n",
    "detect(dali_pytorch_iterator, 'output.mp4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db2fe2cc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "Video('output.mp4')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
